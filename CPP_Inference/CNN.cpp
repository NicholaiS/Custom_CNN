#include <iostream>
#include <cmath>
#include <vector>
#include "Filters_Weights.h"

using namespace std;

// Test function:
void printAsMatrix(float matrix[], int width, int height)
{
    for (int y = 0; y < height; ++y)
    {
        for (int x = 0; x < width; ++x)
        {
            if (matrix[y * width + x] == 0)
            {
                cout << "0.00000" << " ";
            }
            else
            {
                cout << matrix[y * width + x] << " ";
            }
        }
        cout << endl;
    }
}

float max(float a, float b, float c, float d)
{
    if (a > b && a > c && a > d)
        return a;

    else if (b > c && b > d)
        return b;

    else if (c > d)
        return c;

    return d;
}

void maxPooling(float arr[], int w, int h, float out[]) {
    int t = 0;
    for (int i = 0; i < h - 1; i += 2) {
        for (int j = 0; j < w - 1; j += 2) {
            // Check if the next pooling window goes beyond the width or height
            int width_limit = (j + 1 < w - 1) ? j + 1 : w - 1;
            int height_limit = (i + 1 < h - 1) ? i + 1 : h - 1;

            out[t++] = max(
                arr[i * w + j], arr[(i + 1) * w + j],
                arr[i * w + width_limit], arr[height_limit * w + width_limit]
            );
        }
    }
}

void convOperation(float arr[], float filter[], int w, int h, float out[], float bias)
{
    int t = 0;
    float sum;
    for (int i = 1; i < h - 1; i++)
    {
        for (int j = 1; j < w - 1; j++)
        {
            sum = bias;
            for (int m = 0; m < 3; m++)
            {
                for (int n = 0; n < 3; n++)
                {
                    sum += arr[(i - 1 + m) * w + (j - 1 + n)] * filter[m * 3 + n];
                }
            }
            out[t++] = sum;
        }
    }
}

void fullyConnected(float in[], int size_in, const float weights[], int size_w, float out[])
{
    float sum;
    float w = 0;
    float inn = 0;
    for (int i = 0; i < size_w; i++)
    {
        sum = 0;
        for (int j = 0; j < size_in; j++)
        {
            w = weights[(size_w * j) + i];
            inn = in[j];
            sum += w * inn;
        }
        out[i] = sum;
    }
    return;
}

void addBias(float vec[], float bias[], int size)
{
    for (int i = 0; i < size; i++)
    {
        vec[i] += bias[i];
    }
}

void relu(float in[], float out[], int size) 
{
    for (int i = 0; i < size; i++) 
    {
        out[i] = (in[i] >= 0) ? in[i] : 0;
    }
}

void softmax(float scores[], int size) 
{
    float max_score = scores[0];
    for (int i = 1; i < size; ++i) 
    {
        if (scores[i] > max_score) 
        {
            max_score = scores[i];
        }
    }

    float sum = 0.0;
    for (int i = 0; i < size; ++i) 
    {
        scores[i] = exp(scores[i] - max_score);
        sum += scores[i];
    }

    for (int i = 0; i < size; ++i) 
    {
        scores[i] /= sum;
    }
}

void inference(float img[], float out[4])
{
    cout << "Running inference" << endl;

    const int height0 = 30;
    const int width0 = 40;
    const int height0_conv = height0 - 2;
    const int width0_conv = width0 - 2;
    const int height1 = height0_conv / 2;
    const int width1 = width0_conv / 2;
    const int height1_conv = height1 - 2;
    const int width1_conv = width1 - 2;
    const int height2 = height1_conv / 2;
    const int width2 = width1_conv / 2;
    const int height2_conv = height2 - 2;
    const int width2_conv = width2 - 2;
    const int height_last = height2_conv / 2;
    const int width_last = width2_conv / 2;

    float conv_out[height0 * width0] = {};
    float pool_out[height0 * width0] = {};

    // --------------------------------- Convolutional Layers -----------------------------------------

    convOperation(img, layer_0, width0, height0, conv_out, bias_conv_0);
    relu(conv_out, conv_out, width0 * height0);
    maxPooling(conv_out, width0_conv, height0_conv, pool_out);

    convOperation(pool_out, layer_1, width1, height1, conv_out, bias_conv_1);
    relu(conv_out, conv_out, width1 * height1);
    maxPooling(conv_out, width1_conv, height1_conv, pool_out);

    convOperation(pool_out, layer_2, width2, height2, conv_out, bias_conv_2);
    relu(conv_out, conv_out, width2 * height2);
    maxPooling(conv_out, width2_conv, height2_conv, pool_out);


    // --------------------------------- Fully-Connected Layer -----------------------------------------
    int weights_0_size = 64;
    int weights_1_size = 32;
    int weights_2_size = 12;
    int weights_3_size = 4;

    fullyConnected(pool_out, width_last * height_last, weights_0, weights_0_size, conv_out);
    addBias(conv_out, bias_layer0, weights_0_size);
    relu(conv_out, conv_out, weights_0_size);

    fullyConnected(conv_out, weights_0_size, weights_1, weights_1_size, pool_out);
    addBias(pool_out, bias_layer1, weights_1_size);
    relu(pool_out, pool_out, weights_1_size);

    fullyConnected(pool_out, weights_1_size, weights_2, weights_2_size, conv_out);
    addBias(conv_out, bias_layer2, weights_2_size);
    relu(conv_out, conv_out, weights_2_size);

    fullyConnected(conv_out, weights_2_size, weights_3, weights_3_size, pool_out);
    addBias(pool_out, bias_layer3, weights_3_size);
    out[0] = pool_out[0];
    out[1] = pool_out[1];
    out[2] = pool_out[2];
    out[3] = pool_out[3];
}

int main() 
{
    float out[4];

    //Class 3: (CPP Gæt: 3 / Py Gæt: 3)
    float image[] = { 0.368627,0.356863,0.376471,0.368627,0.368627,0.368627,0.360784,0.352941,0.352941,0.356863,0.360784,0.372549,0.380392,0.380392,0.380392,0.384314,0.400000,0.392157,0.396078,0.403922,0.419608,0.423529,0.427451,0.423529,0.423529,0.447059,0.435294,0.450980,0.443137,0.447059,0.447059,0.454902,0.443137,0.470588,0.462745,0.454902,0.462745,0.458824,0.470588,0.458824,0.360784,0.368627,0.368627,0.360784,0.360784,0.352941,0.364706,0.364706,0.360784,0.364706,0.352941,0.364706,0.364706,0.372549,0.388235,0.384314,0.388235,0.392157,0.392157,0.400000,0.443137,0.427451,0.427451,0.427451,0.431373,0.435294,0.439216,0.458824,0.447059,0.450980,0.447059,0.443137,0.454902,0.462745,0.458824,0.462745,0.474510,0.470588,0.458824,0.462745,0.372549,0.372549,0.368627,0.372549,0.360784,0.349020,0.360784,0.364706,0.356863,0.352941,0.352941,0.356863,0.368627,0.360784,0.372549,0.380392,0.384314,0.400000,0.411765,0.450980,0.439216,0.439216,0.423529,0.431373,0.439216,0.443137,0.435294,0.439216,0.454902,0.450980,0.462745,0.458824,0.462745,0.466667,0.466667,0.466667,0.470588,0.462745,0.474510,0.470588,0.376471,0.368627,0.376471,0.360784,0.364706,0.364706,0.360784,0.356863,0.360784,0.352941,0.345098,0.356863,0.360784,0.356863,0.352941,0.368627,0.380392,0.419608,0.945098,1.000000,1.000000,0.556863,0.431373,0.443137,0.431373,0.439216,0.454902,0.439216,0.447059,0.447059,0.458824,0.462745,0.435294,0.458824,0.466667,0.462745,0.462745,0.466667,0.470588,0.466667,0.376471,0.364706,0.364706,0.364706,0.352941,0.360784,0.356863,0.352941,0.356863,0.345098,0.356863,0.352941,0.360784,0.356863,0.368627,0.396078,0.858824,1.000000,1.000000,1.000000,1.000000,1.000000,0.976471,0.454902,0.439216,0.447059,0.454902,0.470588,0.462745,0.458824,0.443137,0.458824,0.454902,0.443137,0.450980,0.466667,0.462745,0.462745,0.462745,0.462745,0.364706,0.372549,0.372549,0.364706,0.356863,0.356863,0.356863,0.352941,0.349020,0.356863,0.360784,0.368627,0.368627,0.384314,0.800000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.933333,0.443137,0.435294,0.450980,0.454902,0.450980,0.458824,0.458824,0.470588,0.466667,0.462745,0.474510,0.474510,0.482353,0.478431,0.466667,0.474510,0.372549,0.372549,0.364706,0.364706,0.368627,0.356863,0.360784,0.352941,0.360784,0.360784,0.356863,0.376471,0.745098,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.992157,0.439216,0.458824,0.458824,0.470588,0.466667,0.482353,0.466667,0.478431,0.466667,0.470588,0.474510,0.474510,0.470588,0.470588,0.474510,0.474510,0.364706,0.368627,0.364706,0.360784,0.356863,0.360784,0.360784,0.356863,0.356863,0.368627,0.686275,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.760784,0.968627,0.666667,0.470588,0.474510,0.470588,0.466667,0.478431,0.486275,0.482353,0.501961,0.486275,0.490196,0.482353,0.478431,0.372549,0.376471,0.376471,0.364706,0.368627,0.360784,0.356863,0.352941,0.368627,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,0.984314,0.505882,0.490196,0.486275,0.466667,0.486275,0.474510,0.494118,0.490196,0.494118,0.478431,0.474510,0.376471,0.376471,0.372549,0.380392,0.372549,0.364706,0.356863,0.356863,0.427451,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.996078,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.525490,0.490196,0.474510,0.474510,0.462745,0.478431,0.470588,0.478431,0.482353,0.466667,0.372549,0.384314,0.384314,0.372549,0.360784,0.376471,0.356863,0.364706,0.435294,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.996078,0.996078,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.635294,0.498039,0.478431,0.490196,0.466667,0.470588,0.482353,0.486275,0.498039,0.470588,0.384314,0.388235,0.376471,0.368627,0.372549,0.364706,0.349020,0.360784,0.466667,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.992157,0.996078,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.564706,0.501961,0.498039,0.498039,0.482353,0.478431,0.494118,0.494118,0.470588,0.478431,0.376471,0.388235,0.372549,0.380392,0.360784,0.352941,0.364706,0.380392,0.521569,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.992157,0.996078,0.996078,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.541176,0.498039,0.517647,0.517647,0.478431,0.494118,0.482353,0.501961,0.494118,0.498039,0.376471,0.368627,0.360784,0.364706,0.392157,0.380392,0.372549,0.368627,0.572549,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.992157,0.996078,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.521569,0.509804,0.498039,0.482353,0.501961,0.498039,0.474510,0.498039,0.498039,0.486275,0.364706,0.396078,0.396078,0.388235,0.392157,0.380392,0.372549,0.372549,0.627451,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.992157,0.996078,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.521569,0.517647,0.498039,0.498039,0.498039,0.501961,0.490196,0.498039,0.494118,0.509804,0.396078,0.388235,0.400000,0.392157,0.380392,0.376471,0.384314,0.376471,0.698039,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.996078,0.996078,0.996078,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.545098,0.509804,0.509804,0.490196,0.486275,0.498039,0.498039,0.490196,0.505882,0.513725,0.400000,0.392157,0.400000,0.400000,0.392157,0.384314,0.380392,0.368627,0.780392,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.996078,0.996078,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.537255,0.501961,0.521569,0.494118,0.509804,0.498039,0.509804,0.509804,0.509804,0.509804,0.403922,0.396078,0.396078,0.400000,0.396078,0.388235,0.376471,0.372549,0.811765,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.992157,0.996078,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.525490,0.513725,0.501961,0.505882,0.509804,0.517647,0.525490,0.513725,0.509804,0.509804,0.403922,0.407843,0.400000,0.400000,0.388235,0.388235,0.392157,0.388235,0.858824,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.996078,0.996078,1.000000,0.996078,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.537255,0.517647,0.505882,0.521569,0.509804,0.513725,0.513725,0.498039,0.505882,0.517647,0.403922,0.411765,0.400000,0.411765,0.400000,0.388235,0.392157,0.396078,0.898039,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.537255,0.509804,0.521569,0.505882,0.509804,0.509804,0.513725,0.529412,0.529412,0.541176,0.419608,0.407843,0.415686,0.396078,0.403922,0.400000,0.396078,0.407843,0.815686,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.529412,0.517647,0.509804,0.505882,0.509804,0.525490,0.501961,0.541176,0.529412,0.537255,0.403922,0.423529,0.419608,0.419608,0.415686,0.396078,0.392157,0.407843,0.407843,0.749020,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.972549,0.521569,0.509804,0.517647,0.509804,0.513725,0.521569,0.501961,0.517647,0.541176,0.513725,0.407843,0.407843,0.407843,0.403922,0.411765,0.403922,0.403922,0.407843,0.407843,0.415686,0.443137,1.000000,0.988235,0.435294,0.984314,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.976471,0.525490,0.505882,0.509804,0.509804,0.537255,0.521569,0.513725,0.537255,0.537255,0.545098,0.537255,0.400000,0.411765,0.403922,0.403922,0.400000,0.396078,0.403922,0.400000,0.392157,0.411765,0.411765,0.431373,0.431373,0.439216,0.439216,0.772549,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.937255,0.521569,0.517647,0.533333,0.521569,0.521569,0.525490,0.517647,0.513725,0.517647,0.529412,0.545098,0.537255,0.525490,0.407843,0.407843,0.403922,0.411765,0.396078,0.396078,0.400000,0.396078,0.419608,0.427451,0.411765,0.423529,0.431373,0.427451,0.443137,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.905882,0.533333,0.521569,0.556863,0.513725,0.529412,0.525490,0.517647,0.533333,0.521569,0.521569,0.533333,0.529412,0.537255,0.545098,0.552941,0.423529,0.411765,0.411765,0.403922,0.396078,0.419608,0.423529,0.415686,0.423529,0.415686,0.419608,0.427451,0.435294,0.431373,0.450980,0.756863,1.000000,1.000000,1.000000,1.000000,1.000000,1.000000,0.819608,0.490196,0.509804,0.509804,0.498039,0.517647,0.517647,0.513725,0.533333,0.513725,0.525490,0.533333,0.533333,0.533333,0.549020,0.537255,0.537255,0.545098,0.400000,0.407843,0.415686,0.411765,0.403922,0.415686,0.427451,0.396078,0.423529,0.423529,0.431373,0.431373,0.431373,0.431373,0.439216,0.450980,0.478431,0.996078,1.000000,1.000000,0.843137,0.505882,0.498039,0.501961,0.513725,0.505882,0.517647,0.505882,0.501961,0.513725,0.521569,0.517647,0.521569,0.529412,0.525490,0.533333,0.552941,0.529412,0.545098,0.556863,0.407843,0.396078,0.415686,0.419608,0.403922,0.411765,0.419608,0.415686,0.419608,0.419608,0.435294,0.435294,0.450980,0.427451,0.427451,0.458824,0.454902,0.466667,0.490196,0.498039,0.482353,0.486275,0.478431,0.505882,0.509804,0.505882,0.505882,0.505882,0.501961,0.517647,0.517647,0.521569,0.545098,0.549020,0.533333,0.552941,0.537255,0.556863,0.545098,0.552941,0.415686,0.415686,0.423529,0.423529,0.419608,0.415686,0.411765,0.411765,0.427451,0.431373,0.458824,0.439216,0.439216,0.427451,0.431373,0.450980,0.443137,0.462745,0.474510,0.486275,0.462745,0.470588,0.498039,0.498039,0.505882,0.501961,0.509804,0.513725,0.509804,0.517647,0.521569,0.529412,0.525490,0.537255,0.529412,0.541176,0.560784,0.552941,0.556863,0.552941,0.419608,0.419608,0.415686,0.415686,0.419608,0.411765,0.423529,0.423529,0.423529,0.443137,0.439216,0.443137,0.443137,0.435294,0.439216,0.454902,0.458824,0.466667,0.454902,0.466667,0.474510,0.470588,0.478431,0.490196,0.501961,0.501961,0.494118,0.501961,0.509804,0.541176,0.509804,0.513725,0.537255,0.541176,0.545098,0.541176,0.552941,0.560784,0.576471,0.564706 };

    inference(image, out);

    softmax(out, 4);

    std::cout << "Class prediction probabilities:" << std::endl;
    std::cout << "Class_0: " << out[0] << std::endl;
    std::cout << "Class_1: " << out[1] << std::endl;
    std::cout << "Class_2: " << out[2] << std::endl;
    std::cout << "Class_3: " << out[3] << std::endl;

    return 0;
}